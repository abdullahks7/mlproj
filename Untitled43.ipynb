{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO53LM+gJx8xocnEjFMIqp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullahks7/mlproj/blob/main/Untitled43.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq39S4iNr-_Z",
        "outputId": "cd4295ce-4dee-46a1-d234-d615a4ee29b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q \"numpy<2.0\"\n",
        "\n",
        "!pip install -q \\\n",
        "  torch torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "!pip install -q \\\n",
        "  speechbrain \\\n",
        "  \"transformers[audio]==4.45.2\" \\\n",
        "  \"datasets==2.18.0\" \\\n",
        "  \"modelscope==1.14.0\" \\\n",
        "  soundfile\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"L2-normalized cosine similarity.\"\"\"\n",
        "    a = a.astype(np.float32)\n",
        "    b = b.astype(np.float32)\n",
        "    a = a / (np.linalg.norm(a) + 1e-6)\n",
        "    b = b / (np.linalg.norm(b) + 1e-6)\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "\n",
        "def verdict_from_cosine(cos: float, threshold: float = 0.6) -> str:\n",
        "    \"\"\"\n",
        "    Simple rule:\n",
        "      - cos >= threshold  -> SAME speaker\n",
        "      - cos < threshold   -> DIFFERENT speaker\n",
        "    \"\"\"\n",
        "    if cos is None:\n",
        "        return \"n/a\"\n",
        "    return \"same\" if cos >= threshold else \"different\"\n",
        "\n",
        "\n",
        "def crop_center(wav: torch.Tensor, sr: int, max_duration_s: Optional[float]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Center-crop waveform to max_duration_s if longer.\n",
        "    wav: [1, T]\n",
        "    \"\"\"\n",
        "    if max_duration_s is None:\n",
        "        return wav\n",
        "    max_len = int(sr * max_duration_s)\n",
        "    T = wav.shape[-1]\n",
        "    if T <= max_len:\n",
        "        return wav\n",
        "    start = (T - max_len) // 2\n",
        "    return wav[..., start:start + max_len]\n",
        "\n",
        "\n",
        "def load_wave(\n",
        "    path: str,\n",
        "    target_sr: Optional[int] = 16000,\n",
        "    device: str = \"cpu\",\n",
        "    max_duration_s: Optional[float] = None,\n",
        ") -> Tuple[torch.Tensor, int]:\n",
        "    \"\"\"\n",
        "    Load audio with torchaudio, convert to mono, resample to target_sr,\n",
        "    and optionally center-crop to max_duration_s.\n",
        "\n",
        "    Handles WAV, MP3, etc. as long as torchaudio + ffmpeg are available.\n",
        "    Returns (waveform [1, T] on device, sr).\n",
        "    \"\"\"\n",
        "    import torchaudio\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Audio file not found: {path}\")\n",
        "\n",
        "    # torchaudio.load works for .wav, .mp3, etc.\n",
        "    wav, sr = torchaudio.load(path)  # [channels, time]\n",
        "\n",
        "    # Ensure [1, time]\n",
        "    if wav.dim() == 1:\n",
        "        wav = wav.unsqueeze(0)\n",
        "\n",
        "    # Convert to mono if needed\n",
        "    if wav.shape[0] > 1:\n",
        "        wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "    # Resample if needed\n",
        "    if target_sr is not None and sr != target_sr:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
        "        wav = resampler(wav)\n",
        "        sr = target_sr\n",
        "\n",
        "    # Center-crop\n",
        "    wav = crop_center(wav, sr, max_duration_s)\n",
        "\n",
        "    # Ensure float32\n",
        "    wav = wav.to(device=device, dtype=torch.float32)\n",
        "    return wav, sr\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# Model caches (no re-download)\n",
        "# ===============================\n",
        "\n",
        "_speechbrain_cache: Dict[Tuple[str, str], Any] = {}\n",
        "_hf_cache: Dict[Tuple[str, str], Dict[str, Any]] = {}\n",
        "\n",
        "\n",
        "def get_speechbrain_classifier(source: str, device: str = \"cpu\"):\n",
        "    \"\"\"Cache SpeechBrain classifiers per (source, device).\"\"\"\n",
        "    from speechbrain.inference.speaker import EncoderClassifier\n",
        "\n",
        "    key = (source, device)\n",
        "    if key not in _speechbrain_cache:\n",
        "        _speechbrain_cache[key] = EncoderClassifier.from_hparams(\n",
        "            source=source,\n",
        "            run_opts={\"device\": device},\n",
        "        )\n",
        "    return _speechbrain_cache[key]\n",
        "\n",
        "\n",
        "def get_hf_model(\n",
        "    model_name: str,\n",
        "    feature_extractor_cls,\n",
        "    model_cls,\n",
        "    device: str = \"cpu\",\n",
        "):\n",
        "    \"\"\"Cache HF feature_extractor + model per (model_name, device).\"\"\"\n",
        "    key = (model_name, device)\n",
        "    if key not in _hf_cache:\n",
        "        feature_extractor = feature_extractor_cls.from_pretrained(model_name)\n",
        "        model = model_cls.from_pretrained(model_name).to(device)\n",
        "        model.eval()\n",
        "        _hf_cache[key] = {\n",
        "            \"feature_extractor\": feature_extractor,\n",
        "            \"model\": model,\n",
        "        }\n",
        "    return _hf_cache[key][\"feature_extractor\"], _hf_cache[key][\"model\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----- SpeechBrain models -----\n",
        "\n",
        "def ecapa_embedding(path: str, device: str = \"cpu\", max_duration_s: Optional[float] = 5.0) -> np.ndarray:\n",
        "    classifier = get_speechbrain_classifier(\"speechbrain/spkrec-ecapa-voxceleb\", device)\n",
        "    wav, _ = load_wave(path, target_sr=16000, device=device, max_duration_s=max_duration_s)\n",
        "    emb = classifier.encode_batch(wav)  # [batch, 1, emb_dim] or [batch, emb_dim]\n",
        "    emb = emb.squeeze().detach().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "def resnet_embedding(path: str, device: str = \"cpu\", max_duration_s: Optional[float] = 5.0) -> np.ndarray:\n",
        "    classifier = get_speechbrain_classifier(\"speechbrain/spkrec-resnet-voxceleb\", device)\n",
        "    wav, _ = load_wave(path, target_sr=16000, device=device, max_duration_s=max_duration_s)\n",
        "    emb = classifier.encode_batch(wav).squeeze().detach().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "def xvect_embedding(path: str, device: str = \"cpu\", max_duration_s: Optional[float] = 5.0) -> np.ndarray:\n",
        "    classifier = get_speechbrain_classifier(\"speechbrain/spkrec-xvect-voxceleb\", device)\n",
        "    wav, _ = load_wave(path, target_sr=16000, device=device, max_duration_s=max_duration_s)\n",
        "    emb = classifier.encode_batch(wav).squeeze().detach().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "# ----- HF x-vector SV models -----\n",
        "\n",
        "def wavlm_sv_embedding(path: str, device: str = \"cpu\", max_duration_s: Optional[float] = 5.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    microsoft/wavlm-base-plus-sv using WavLMForXVector.\n",
        "    \"\"\"\n",
        "    from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
        "\n",
        "    model_name = \"microsoft/wavlm-base-plus-sv\"\n",
        "    fe, model = get_hf_model(model_name, Wav2Vec2FeatureExtractor, WavLMForXVector, device)\n",
        "\n",
        "    wav, _ = load_wave(path, target_sr=16000, device=device, max_duration_s=max_duration_s)\n",
        "    wav_np = wav.squeeze().cpu().numpy()\n",
        "\n",
        "    inputs = fe(\n",
        "        [wav_np],\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        emb = outputs.embeddings  # [batch, emb_dim]\n",
        "\n",
        "    emb = emb.squeeze().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "def unispeech_sv_embedding(path: str, device: str = \"cpu\", max_duration_s: Optional[float] = 5.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    microsoft/unispeech-sat-base-plus-sv using UniSpeechSatForXVector.\n",
        "    \"\"\"\n",
        "    from transformers import Wav2Vec2FeatureExtractor, UniSpeechSatForXVector\n",
        "\n",
        "    model_name = \"microsoft/unispeech-sat-base-plus-sv\"\n",
        "    fe, model = get_hf_model(model_name, Wav2Vec2FeatureExtractor, UniSpeechSatForXVector, device)\n",
        "\n",
        "    wav, _ = load_wave(path, target_sr=16000, device=device, max_duration_s=max_duration_s)\n",
        "    wav_np = wav.squeeze().cpu().numpy()\n",
        "\n",
        "    inputs = fe(\n",
        "        [wav_np],\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        emb = outputs.embeddings  # [batch, emb_dim]\n",
        "\n",
        "    emb = emb.squeeze().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "def wav2vec2_superb_embedding(path: str, device: str = \"cpu\", max_duration_s: Optional[float] = 5.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    anton-l/wav2vec2-base-superb-sv using Wav2Vec2ForXVector.\n",
        "    \"\"\"\n",
        "    from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForXVector\n",
        "\n",
        "    model_name = \"anton-l/wav2vec2-base-superb-sv\"\n",
        "    fe, model = get_hf_model(model_name, Wav2Vec2FeatureExtractor, Wav2Vec2ForXVector, device)\n",
        "\n",
        "    wav, _ = load_wave(path, target_sr=16000, device=device, max_duration_s=max_duration_s)\n",
        "    wav_np = wav.squeeze().cpu().numpy()\n",
        "\n",
        "    inputs = fe(\n",
        "        [wav_np],\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        emb = outputs.embeddings  # [batch, emb_dim]\n",
        "\n",
        "    emb = emb.squeeze().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "# ----- HF generic encoders (pooled) -----\n",
        "\n",
        "def wavlm_base_pooled_embedding(path: str, device: str = \"cpu\", max_duration_s: Optional[float] = 5.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    microsoft/wavlm-base as a generic encoder:\n",
        "    mean-pool last_hidden_state over time.\n",
        "    \"\"\"\n",
        "    from transformers import AutoFeatureExtractor, WavLMModel\n",
        "\n",
        "    model_name = \"microsoft/wavlm-base\"\n",
        "    fe, model = get_hf_model(model_name, AutoFeatureExtractor, WavLMModel, device)\n",
        "\n",
        "    wav, _ = load_wave(path, target_sr=16000, device=device, max_duration_s=max_duration_s)\n",
        "    wav_np = wav.squeeze().cpu().numpy()\n",
        "\n",
        "    inputs = fe(\n",
        "        [wav_np],\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        hidden = outputs.last_hidden_state  # [B, T, C]\n",
        "        emb = hidden.mean(dim=1)            # [B, C]\n",
        "\n",
        "    emb = emb.squeeze().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "def wav2vec2_base_pooled_embedding(path: str, device: str = \"cpu\", max_duration_s: Optional[float] = 5.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    facebook/wav2vec2-base as a generic encoder:\n",
        "    mean-pool last_hidden_state over time.\n",
        "    \"\"\"\n",
        "    from transformers import AutoFeatureExtractor, Wav2Vec2Model\n",
        "\n",
        "    model_name = \"facebook/wav2vec2-base\"\n",
        "    fe, model = get_hf_model(model_name, AutoFeatureExtractor, Wav2Vec2Model, device)\n",
        "\n",
        "    wav, _ = load_wave(path, target_sr=16000, device=device, max_duration_s=max_duration_s)\n",
        "    wav_np = wav.squeeze().cpu().numpy()\n",
        "\n",
        "    inputs = fe(\n",
        "        [wav_np],\n",
        "        sampling_rate=16000,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        hidden = outputs.last_hidden_state  # [B, T, C]\n",
        "        emb = hidden.mean(dim=1)\n",
        "\n",
        "    emb = emb.squeeze().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_benchmark(\n",
        "    audio1: str,\n",
        "    audio2: str,\n",
        "    device: Optional[str] = None,\n",
        "    threshold: float = 0.6,\n",
        "    max_duration_s: Optional[float] = 5.0,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Run multiple speaker models on two audio files and print a comparison table.\n",
        "    Returns a list of result dicts.\n",
        "\n",
        "    threshold: cosine threshold for same/different decision.\n",
        "    max_duration_s: audio is center-cropped to this many seconds before embedding.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Audio1: {audio1}\")\n",
        "    print(f\"Audio2: {audio2}\")\n",
        "    print(f\"Decision threshold: {threshold}\")\n",
        "    print(f\"Max duration (crop): {max_duration_s}s\")\n",
        "    print()\n",
        "\n",
        "    if not os.path.exists(audio1):\n",
        "        raise FileNotFoundError(f\"audio1 not found: {audio1}\")\n",
        "    if not os.path.exists(audio2):\n",
        "        raise FileNotFoundError(f\"audio2 not found: {audio2}\")\n",
        "\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    def run_model(name: str, fn, kind: str):\n",
        "        \"\"\"Run embedding+cosine model safely and compute extra metrics.\"\"\"\n",
        "        try:\n",
        "            emb1 = fn(audio1, device=device, max_duration_s=max_duration_s)\n",
        "            emb2 = fn(audio2, device=device, max_duration_s=max_duration_s)\n",
        "\n",
        "            # Raw norms\n",
        "            norm1 = float(np.linalg.norm(emb1) + 1e-6)\n",
        "            norm2 = float(np.linalg.norm(emb2) + 1e-6)\n",
        "\n",
        "            # Normalized embeddings\n",
        "            emb1n = emb1 / norm1\n",
        "            emb2n = emb2 / norm2\n",
        "\n",
        "            # Cosine similarity (dot of normalized)\n",
        "            cos = float(np.dot(emb1n.astype(np.float32), emb2n.astype(np.float32)))\n",
        "\n",
        "            # L2 distance between normalized vectors\n",
        "            l2dist = float(np.linalg.norm(emb1n - emb2n))\n",
        "\n",
        "            verdict = verdict_from_cosine(cos, threshold=threshold)\n",
        "\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"model\": name,\n",
        "                    \"type\": kind,\n",
        "                    \"cosine\": cos,\n",
        "                    \"l2dist\": l2dist,\n",
        "                    \"norm1\": norm1,\n",
        "                    \"norm2\": norm2,\n",
        "                    \"verdict\": verdict,\n",
        "                    \"extra\": \"\",\n",
        "                }\n",
        "            )\n",
        "        except Exception as e:\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"model\": name,\n",
        "                    \"type\": kind,\n",
        "                    \"cosine\": None,\n",
        "                    \"l2dist\": None,\n",
        "                    \"norm1\": None,\n",
        "                    \"norm2\": None,\n",
        "                    \"verdict\": \"error\",\n",
        "                    \"extra\": f\"ERROR: {repr(e)}\",\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # üîπ SpeechBrain models\n",
        "    run_model(\"ecapa-voxceleb\",       ecapa_embedding,         \"speechbrain-sv\")\n",
        "    run_model(\"resnet-voxceleb\",      resnet_embedding,        \"speechbrain-sv\")\n",
        "    run_model(\"xvect-voxceleb\",       xvect_embedding,         \"speechbrain-sv\")\n",
        "\n",
        "    # üîπ HF x-vector SV models\n",
        "    run_model(\"wavlm-base-plus-sv\",   wavlm_sv_embedding,      \"hf-xvector-sv\")\n",
        "    run_model(\"unispeech-sat-plus-sv\",unispeech_sv_embedding,  \"hf-xvector-sv\")\n",
        "    run_model(\"wav2vec2-superb-sv\",   wav2vec2_superb_embedding,\"hf-xvector-sv\")\n",
        "\n",
        "    # üîπ HF generic encoders (pooled)\n",
        "    run_model(\"wavlm-base-pooled\",    wavlm_base_pooled_embedding, \"hf-generic-pooled\")\n",
        "    run_model(\"wav2vec2-base-pooled\", wav2vec2_base_pooled_embedding,\"hf-generic-pooled\")\n",
        "\n",
        "    # Pretty print\n",
        "    print(\"Results:\\n\")\n",
        "    print(\n",
        "        f\"{'Model':24} {'Kind':20} {'Cosine':10} {'L2dist':10} \"\n",
        "        f\"{'Norm1':10} {'Norm2':10} {'Verdict':10} Extra\"\n",
        "    )\n",
        "    print(\"-\" * 130)\n",
        "    for r in rows:\n",
        "        cosine_str = f\"{r['cosine']:.4f}\" if isinstance(r[\"cosine\"], float) else \"None\"\n",
        "        l2_str     = f\"{r['l2dist']:.4f}\" if isinstance(r[\"l2dist\"], float) else \"None\"\n",
        "        n1_str     = f\"{r['norm1']:.2f}\" if isinstance(r[\"norm1\"], float) else \"None\"\n",
        "        n2_str     = f\"{r['norm2']:.2f}\" if isinstance(r[\"norm2\"], float) else \"None\"\n",
        "\n",
        "        print(\n",
        "            f\"{r['model']:24} {r['type']:20} {cosine_str:10} {l2_str:10} \"\n",
        "            f\"{n1_str:10} {n2_str:10} {r['verdict']:10} {r['extra']}\"\n",
        "        )\n",
        "\n",
        "    # Majority verdict (ignoring errors)\n",
        "    valid = [r for r in rows if isinstance(r[\"cosine\"], float)]\n",
        "    same_count = sum(1 for r in valid if r[\"verdict\"] == \"same\")\n",
        "    diff_count = sum(1 for r in valid if r[\"verdict\"] == \"different\")\n",
        "\n",
        "    if valid:\n",
        "        print(\"\\n--- Overall ---\")\n",
        "        if same_count > diff_count:\n",
        "            overall = \"SAME SPEAKER\"\n",
        "        elif diff_count > same_count:\n",
        "            overall = \"DIFFERENT SPEAKERS\"\n",
        "        else:\n",
        "            overall = \"UNCERTAIN / TIE\"\n",
        "\n",
        "        print(\n",
        "            f\"Majority verdict (@threshold={threshold}): {overall} \"\n",
        "            f\"(same={same_count}, different={diff_count}, total_valid={len(valid)})\"\n",
        "        )\n",
        "\n",
        "    return rows\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iirOwEXt6y5",
        "outputId": "ffc7486e-ca29-4423-a8c3-b35e3b011be0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading SpeechBrain models...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/hyperparams.yaml' -> '/content/pretrained_models/ecapa/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in pretrained_models/ecapa.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/embedding_model.ckpt' -> '/content/pretrained_models/ecapa/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /content/pretrained_models/ecapa/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/mean_var_norm_emb.ckpt' -> '/content/pretrained_models/ecapa/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /content/pretrained_models/ecapa/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/classifier.ckpt' -> '/content/pretrained_models/ecapa/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /content/pretrained_models/ecapa/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-ecapa-voxceleb/snapshots/0f99f2d0ebe89ac095bcc5903c4dd8f72b367286/label_encoder.txt' -> '/content/pretrained_models/ecapa/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /content/pretrained_models/ecapa/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /content/pretrained_models/ecapa/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /content/pretrained_models/ecapa/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /content/pretrained_models/ecapa/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /content/pretrained_models/ecapa/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /content/pretrained_models/ecapa/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-resnet-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-resnet-voxceleb/snapshots/be9f369e6bb16183244f4c47c7251f447e7babca/hyperparams.yaml' -> '/content/pretrained_models/resnet/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-resnet-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in pretrained_models/resnet.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'underdogliu1005/spkrec-resnet-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--underdogliu1005--spkrec-resnet-voxceleb/snapshots/2986f22a0fee4937db66d4ad3011785f6657fd2a/embedding_model.ckpt' -> '/content/pretrained_models/resnet/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /content/pretrained_models/resnet/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'underdogliu1005/spkrec-resnet-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--underdogliu1005--spkrec-resnet-voxceleb/snapshots/2986f22a0fee4937db66d4ad3011785f6657fd2a/classifier.ckpt' -> '/content/pretrained_models/resnet/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /content/pretrained_models/resnet/classifier.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, classifier\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /content/pretrained_models/resnet/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /content/pretrained_models/resnet/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-xvect-voxceleb/snapshots/56895a2df401be4150a159f3a1c653f00051d477/hyperparams.yaml' -> '/content/pretrained_models/xvect/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in pretrained_models/xvect.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-xvect-voxceleb/snapshots/56895a2df401be4150a159f3a1c653f00051d477/embedding_model.ckpt' -> '/content/pretrained_models/xvect/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /content/pretrained_models/xvect/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-xvect-voxceleb/snapshots/56895a2df401be4150a159f3a1c653f00051d477/mean_var_norm_emb.ckpt' -> '/content/pretrained_models/xvect/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /content/pretrained_models/xvect/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-xvect-voxceleb/snapshots/56895a2df401be4150a159f3a1c653f00051d477/classifier.ckpt' -> '/content/pretrained_models/xvect/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /content/pretrained_models/xvect/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
            "DEBUG:speechbrain.utils.fetching:Fetch: Local file found, creating symlink '/root/.cache/huggingface/hub/models--speechbrain--spkrec-xvect-voxceleb/snapshots/56895a2df401be4150a159f3a1c653f00051d477/label_encoder.txt' -> '/content/pretrained_models/xvect/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /content/pretrained_models/xvect/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /content/pretrained_models/xvect/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /content/pretrained_models/xvect/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /content/pretrained_models/xvect/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /content/pretrained_models/xvect/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /content/pretrained_models/xvect/label_encoder.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading HF pooled models (WavLM, Wav2Vec2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 3D-Speaker pipelines (ERes2Net, ERes2NetV2, CAMP++)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-16 10:27:49,531 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n",
            "2025-11-16 10:27:50,291 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/iic/speech_eres2net_base_sv_zh-cn_3dspeaker_16k\n",
            "2025-11-16 10:27:50,293 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/iic/speech_eres2net_base_sv_zh-cn_3dspeaker_16k.\n",
            "2025-11-16 10:27:50,295 - modelscope - INFO - initialize model from /root/.cache/modelscope/hub/iic/speech_eres2net_base_sv_zh-cn_3dspeaker_16k\n",
            "2025-11-16 10:27:50,298 - modelscope - INFO - cuda is not available, using cpu instead.\n",
            "2025-11-16 10:27:50,650 - modelscope - WARNING - No preprocessor field found in cfg.\n",
            "2025-11-16 10:27:50,651 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
            "2025-11-16 10:27:50,652 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/root/.cache/modelscope/hub/iic/speech_eres2net_base_sv_zh-cn_3dspeaker_16k'}. trying to build by task and model information.\n",
            "2025-11-16 10:27:50,653 - modelscope - WARNING - No preprocessor key ('eres2net-sv', 'speaker-verification') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
            "2025-11-16 10:27:50,659 - modelscope - INFO - cuda is not available, using cpu instead.\n",
            "2025-11-16 10:27:52,779 - modelscope - WARNING - Model revision not specified, use revision: v1.0.1\n",
            "2025-11-16 10:27:53,139 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/iic/speech_eres2netv2_sv_zh-cn_16k-common\n",
            "2025-11-16 10:27:53,139 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/iic/speech_eres2netv2_sv_zh-cn_16k-common.\n",
            "2025-11-16 10:27:53,142 - modelscope - INFO - initialize model from /root/.cache/modelscope/hub/iic/speech_eres2netv2_sv_zh-cn_16k-common\n",
            "2025-11-16 10:27:53,144 - modelscope - INFO - cuda is not available, using cpu instead.\n",
            "2025-11-16 10:27:53,688 - modelscope - WARNING - No preprocessor field found in cfg.\n",
            "2025-11-16 10:27:53,689 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
            "2025-11-16 10:27:53,690 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/root/.cache/modelscope/hub/iic/speech_eres2netv2_sv_zh-cn_16k-common'}. trying to build by task and model information.\n",
            "2025-11-16 10:27:53,691 - modelscope - WARNING - No preprocessor key ('eres2netv2-sv', 'speaker-verification') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
            "2025-11-16 10:27:53,693 - modelscope - INFO - cuda is not available, using cpu instead.\n",
            "2025-11-16 10:27:56,016 - modelscope - WARNING - Model revision not specified, use revision: v2.0.2\n",
            "2025-11-16 10:27:56,383 - modelscope - INFO - initiate model from /root/.cache/modelscope/hub/iic/speech_campplus_sv_zh-cn_16k-common\n",
            "2025-11-16 10:27:56,384 - modelscope - INFO - initiate model from location /root/.cache/modelscope/hub/iic/speech_campplus_sv_zh-cn_16k-common.\n",
            "2025-11-16 10:27:56,387 - modelscope - INFO - initialize model from /root/.cache/modelscope/hub/iic/speech_campplus_sv_zh-cn_16k-common\n",
            "2025-11-16 10:27:56,390 - modelscope - INFO - cuda is not available, using cpu instead.\n",
            "2025-11-16 10:27:56,709 - modelscope - WARNING - No preprocessor field found in cfg.\n",
            "2025-11-16 10:27:56,710 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
            "2025-11-16 10:27:56,711 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': '/root/.cache/modelscope/hub/iic/speech_campplus_sv_zh-cn_16k-common'}. trying to build by task and model information.\n",
            "2025-11-16 10:27:56,712 - modelscope - WARNING - No preprocessor key ('cam++-sv', 'speaker-verification') found in PREPROCESSOR_MAP, skip building preprocessor.\n",
            "2025-11-16 10:27:56,713 - modelscope - INFO - cuda is not available, using cpu instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EE634iNu1Jy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}